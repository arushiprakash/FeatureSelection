{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source:** http://www.nltk.org/nltk_data/  \n",
    "Sentiment Polarity Dataset Version 2.0 [ download | source ]\n",
    "id: movie_reviews; size: 4004848; author: Bo Pang and Lillian Lee; copyright: Copyright (C) 2004 Bo Pang and Lillian Lee; license: Creative Commons Attribution 4.0 International;\n",
    "  \n",
    "Following this tutorial: https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_pos = os.listdir('data/pos')\n",
    "files_pos = [open('data/pos/'+f, 'r').read() for f in files_pos if f.endswith('.txt')]\n",
    "files_neg = os.listdir('data/neg')\n",
    "files_neg = [open('data/neg/'+f, 'r').read() for f in files_neg if f.endswith('.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples : 1000; Negative samples : 1000\n",
      "Train samples : 1600; Test samples : 400\n"
     ]
    }
   ],
   "source": [
    "pos_df = pd.DataFrame({'review': files_pos})\n",
    "neg_df = pd.DataFrame({'review': files_neg})\n",
    "print(f'Positive samples : {len(pos_df)}; Negative samples : {len(neg_df)}')\n",
    "\n",
    "pos_train, pos_test = train_test_split(pos_df, test_size=0.2)\n",
    "neg_train, neg_test = train_test_split(neg_df, test_size=0.2)\n",
    "\n",
    "train_df = pd.concat([pos_train, neg_train],ignore_index=True)\n",
    "test_df = pd.concat([pos_test, neg_test],ignore_index=True)\n",
    "print(f'Train samples : {len(train_df)}; Test samples : {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(word_list):\n",
    "    if word_list:\n",
    "        word_list = [token for token in word_list if not token in stop_words]\n",
    "        return word_list\n",
    "    \n",
    "def text_cleaning(df, colname):\n",
    "    df[colname] = df[colname].apply(lambda x: clean_text(x))\n",
    "    df[colname] = df[colname].apply(lambda x: nlp(x))\n",
    "    #df[colname] = df[colname].apply(lambda x: word_tokenize(x))\n",
    "    #df[colname] = df[colname].apply(lambda x: remove_stop_words(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = text_cleaning(train_df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_items = []\n",
    "all_combinations = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    labels = [x.label_ for x in row['review'].ents]\n",
    "    items = [x.text for x in row['review'].ents]\n",
    "    combinations = [(x.text, x.label_) for x in row['review'].ents]\n",
    "    \n",
    "    all_labels.extend(labels)\n",
    "    all_items.extend(items)\n",
    "    all_combinations.extend(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('one', 'CARDINAL'), 2335), (('two', 'CARDINAL'), 1210), (('first', 'ORDINAL'), 1190), (('hollywood', 'GPE'), 431), (('three', 'CARDINAL'), 395), (('american', 'NORP'), 387), (('second', 'ORDINAL'), 272), (('2', 'CARDINAL'), 202), (('four', 'CARDINAL'), 161), (('new york', 'GPE'), 151)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(all_combinations).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PERSON': 18410, 'CARDINAL': 6833, 'DATE': 4428, 'GPE': 3370, 'ORG': 3136, 'NORP': 2338, 'ORDINAL': 1842, 'TIME': 1453, 'PRODUCT': 787, 'LOC': 577, 'QUANTITY': 319, 'FAC': 282, 'EVENT': 187, 'MONEY': 121, 'LANGUAGE': 86, 'WORK_OF_ART': 36, 'LAW': 35, 'PERCENT': 11})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 2335), ('two', 1218), ('first', 1190), ('hollywood', 431), ('three', 399), ('s', 398), ('american', 389), ('second', 272), ('2', 203), ('four', 162)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(all_items).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f9d69e21a93c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "list(FreqDist(sum(pos_df['review'], [])).keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
